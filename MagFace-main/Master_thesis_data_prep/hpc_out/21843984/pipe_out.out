/work3/s174139/Master_Thesis/MagFace-main/Master_thesis_data_prep
Starting fine tuner script...
PATHHHH /work3/s174139/Master_Thesis/MagFace-main
=> parse the args ...
wandb: Currently logged in as: gabriella-angela (fair_face_recognition). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /work3/s174139/Master_Thesis/MagFace-main/Master_thesis_data_prep/wandb/run-20240524_103333-y7wroylq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magface-fine-tuner
wandb: â­ï¸ View project at https://wandb.ai/fair_face_recognition/face-rec-models
wandb: ðŸš€ View run at https://wandb.ai/fair_face_recognition/face-rec-models/runs/y7wroylq
{'arc_scale': 64,
 'arch': 'iresnet18',
 'batch_size': 256,
 'cpu_mode': '0',
 'embedding_size': 512,
 'epochs': 2,
 'l_a': 10.0,
 'l_margin': 0.45,
 'lambda_g': 35.0,
 'last_fc_size': 602,
 'lr': 0.1,
 'lr_drop_epoch': [2],
 'lr_drop_ratio': 0.1,
 'momentum': 0.9,
 'pretrained': '../models/magface_iresnet18_casia_dp.pth',
 'print_freq': 10,
 'pth_save_epoch': 1,
 'pth_save_fold': './test/',
 'start_epoch': 0,
 'train_list': '../../data/data_full/HDA_processed_cluster_magface/fine_tune_train_list_more_hda_magface.list',
 'u_a': 110.0,
 'u_margin': 0.8,
 'vis_mag': 1,
 'weight_decay': 0.0005,
 'workers': 4}
min lambda g is 22.586666666666673, currrent lambda is 35.0
=> torch version : 2.2.1+cu121
=> ngpus : 1
=> modeling the network ...
=> loading pth from ../models/magface_iresnet18_casia_dp.pth ...
cpu!!!
K?  features.weight
new KK 2 fc.weight
=> fc.weight not loaded! Model params: 188, loaded params: 187
=> FREEZING ALL LAYERS EXCEPT fc ...
NAME FC? module.fc.weight
=> building the optimizer ...
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.1
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0005
)
=> building the dataloader ...
TRAIN LOADER???? <torch.utils.data.dataloader.DataLoader object at 0x7f738ef878e0> ARGS: are workers and batch size and train list
=> building the criterion ...
=> starting training engine ...
[codecarbon WARNING @ 10:33:43] Invalid gpu_ids format. Expected a string or a list of ints.
[codecarbon INFO @ 10:33:43] [setup] RAM Tracking...
[codecarbon INFO @ 10:33:43] [setup] GPU Tracking...
[codecarbon INFO @ 10:33:43] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 10:33:43] [setup] CPU Tracking...
[codecarbon WARNING @ 10:33:43] No CPU tracking mode found. Falling back on CPU constant mode.
[codecarbon WARNING @ 10:33:45] We saw that you have a Intel(R) Xeon(R) Gold 6326 CPU @ 2.90GHz but we don't know it. Please contact us.
[codecarbon INFO @ 10:33:45] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6326 CPU @ 2.90GHz
[codecarbon INFO @ 10:33:45] >>> Tracker's metadata:
[codecarbon INFO @ 10:33:45]   Platform system: Linux-5.4.273-1.el7.elrepo.x86_64-x86_64-with-glibc2.17
[codecarbon INFO @ 10:33:45]   Python version: 3.10.13
[codecarbon INFO @ 10:33:45]   CodeCarbon version: 2.4.1
[codecarbon INFO @ 10:33:45]   Available RAM : 1007.365 GB
[codecarbon INFO @ 10:33:45]   CPU count: 32
[codecarbon INFO @ 10:33:45]   CPU model: Intel(R) Xeon(R) Gold 6326 CPU @ 2.90GHz
[codecarbon INFO @ 10:33:45]   GPU count: 1
[codecarbon INFO @ 10:33:45]   GPU model: 1 x NVIDIA A100 80GB PCIe
LEN TRAIN LOADER: 42
--------TENSOR_SHAPE--------
Input batch size: torch.Size([256, 3, 112, 112])
Target batch size: torch.Size([256])
--------TENSOR_SHAPE--------
Input batch size:Input batch size: torch.Size([256, 3, 112, 112])
Target batch size: torch.Size([256])
Output shape: torch.Size([256, 602])
Target shape: torch.Size([256]), Target min: 19122, Target max: 4350031
[codecarbon INFO @ 10:34:05] Energy consumed for RAM : 0.001574 kWh. RAM Power : 377.76174545288086 W
Traceback (most recent call last):
  File "/work3/s174139/Master_Thesis/MagFace-main/Master_thesis_data_prep/../run/fine_tuner.py", line 436, in <module>
    main(args)
  File "/work3/s174139/Master_Thesis/MagFace-main/Master_thesis_data_prep/../run/fine_tuner.py", line 204, in main
    main_worker(args)
  File "/work3/s174139/Master_Thesis/MagFace-main/Master_thesis_data_prep/../run/fine_tuner.py", line 271, in main_worker
    co2_emission, top1, top5, losses_id = do_train(train_loader, model, criterion, optimizer, epoch, args)
  File "/work3/s174139/Master_Thesis/MagFace-main/Master_thesis_data_prep/../run/fine_tuner.py", line 357, in do_train
    assert target.min() >= 0 and target.max() < output[0].size(1), "Target index out of bounds"
AssertionError: Target index out of bounds
[codecarbon INFO @ 10:34:05] Energy consumed for all GPUs : 0.000269 kWh. Total GPU Power : 64.62202668143239 W
[codecarbon INFO @ 10:34:05] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W
[codecarbon INFO @ 10:34:05] 0.002021 kWh of electricity used since the beginning.
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.012 MB uploadedwandb: | 0.012 MB of 0.012 MB uploadedwandb: ðŸš€ View run magface-fine-tuner at: https://wandb.ai/fair_face_recognition/face-rec-models/runs/y7wroylq
wandb: â­ï¸ View project at: https://wandb.ai/fair_face_recognition/face-rec-models
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240524_103333-y7wroylq/logs
